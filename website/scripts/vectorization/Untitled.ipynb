{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71830b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from data_pipeline import TextPreprocessingPipeline\n",
    "\n",
    "    \n",
    "def retrieve_psalm(index, psalms):\n",
    "    # Ensure index is a tuple with (document_name, psalm_number)\n",
    "    doc, psalm_num = index  \n",
    "\n",
    "    print(f\"    Text: {doc}\")\n",
    "    print(f\"    Psalm Number: {psalm_num}\\n\")\n",
    "\n",
    "    # Retrieve and format the verse text as a paragraph\n",
    "    matching_verses = psalms.loc[(psalms['text'] == doc) & (psalms['psalm_num'] == psalm_num), 'verse']\n",
    "\n",
    "    if matching_verses.empty:\n",
    "        print(\"    No matching Psalm found.\")\n",
    "        return\n",
    "\n",
    "    # Removing trailing spaces\n",
    "    verse_text = \" \".join(matching_verses.tolist()).strip()\n",
    "\n",
    "    # Ensure the last full word is displayed within the first 200 characters\n",
    "    if len(verse_text) > 200:\n",
    "        verse_text = verse_text[:200]  # Slice to the first 200 characters\n",
    "        last_space = verse_text.rfind(' ')  # Find the last space in the first 200 characters\n",
    "        verse_text = verse_text[:last_space]  # Trim to the last full word\n",
    "\n",
    "    # Print the first 200 characters (or last full word if it's too long)\n",
    "    print(\"   \" + verse_text + \"...\\n\")\n",
    "\n",
    "\n",
    "def search_psalms(query, pipeline, vectorizer, model, psalms, num_results=6):\n",
    "    # Displaying the query\n",
    "    print(f\"\\033[1mSearching for:\\033[0m {query}.\\n\")\n",
    "    \n",
    "    # Reporting the number of results\n",
    "    print(f\"Top {num_results} results.\")\n",
    "    \n",
    "    # Running the query through the data pipeline\n",
    "    clean_query = pipeline.pipeline(query)\n",
    "\n",
    "    # Transform the query using the loaded vectorizer\n",
    "    clean_vec = vectorizer.transform([clean_query])\n",
    "\n",
    "    # Calculate the cosine similarity between the query vector and the TF-IDF matrix\n",
    "    cosine_similarities = cosine_similarity(clean_vec, model).flatten()\n",
    "    \n",
    "    # Get the indices of the top_n most similar Psalms\n",
    "    top_indices = cosine_similarities.argsort()[-num_results:][::-1]\n",
    "\n",
    "    # For the ranking of the results\n",
    "    n = 1\n",
    "\n",
    "    # Looping through the indices to print them out\n",
    "    for index in top_indices:\n",
    "        # Ensure you have access to both the document name and psalm number from your model\n",
    "        doc, psalm_num = model.index[index]  # Or adjust this part based on your data structure\n",
    "        retrieve_psalm((doc, psalm_num), psalms)  # Pass the tuple (doc, psalm_num) to retrieve_psalm\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get the current working directory and construct the cleaning path\n",
    "    current_dir = os.getcwd()\n",
    "    cleaning_dir = os.path.abspath(os.path.join(current_dir, \"../cleaning\"))\n",
    "\n",
    "    # Add the directory to sys.path\n",
    "    sys.path.append(cleaning_dir)\n",
    "\n",
    "    # Define the directory where pickled files are stored\n",
    "    load_dir = \"../pickles\"\n",
    "\n",
    "    # Load the preprocessed text pipeline\n",
    "    with open(os.path.join(load_dir, \"pipeline.pickle\"), \"rb\") as f:\n",
    "        pipeline = pickle.load(f)\n",
    "\n",
    "    # Load the pickled vectorizer\n",
    "    vectorizer_path = os.path.join(load_dir, \"psalms_tfidf_vectorizer.pickle\")\n",
    "    with open(vectorizer_path, \"rb\") as file:\n",
    "        psalm_vectorizer = pickle.load(file)\n",
    "\n",
    "    # Load the pickled TF-IDF matrix (optional, if you need to load the transformed matrix)\n",
    "    matrix_path = os.path.join(load_dir, \"psalms_tfidf_matrix.pickle\")\n",
    "    with open(matrix_path, \"rb\") as file:\n",
    "        tf_idf_psalms = pickle.load(file)\n",
    "\n",
    "    # Load the cleaned Psalms data from CSV\n",
    "    psalms = pd.read_csv(\"../Data/clean_psalm_verses.csv\")\n",
    "\n",
    "    # Prompt user for search input\n",
    "    query = input(\"Enter text to search the Psalms: \")\n",
    "\n",
    "    # Perform the search with the given query\n",
    "    search_psalms(query, pipeline, psalm_vectorizer, tf_idf_psalms, psalms, num_results=6)\n",
    "\n",
    "\n",
    "# Run the script only if executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
